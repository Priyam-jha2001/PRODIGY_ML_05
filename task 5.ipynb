{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prodigy ML Task 5 ###\n",
    "Develop a model that can accurately recognize food items from images and estimate their calorie content, enabling users to track their dietary intake and make informed food choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 80800 images belonging to 101 classes.\n",
      "Found 20200 images belonging to 101 classes.\n",
      "Epoch 1/20\n",
      "2525/2525 [==============================] - 1982s 784ms/step - loss: 4.5947 - accuracy: 0.0150 - val_loss: 4.5620 - val_accuracy: 0.0171 - lr: 1.0000e-04\n",
      "Epoch 2/20\n",
      "2525/2525 [==============================] - 1951s 773ms/step - loss: 4.5446 - accuracy: 0.0231 - val_loss: 4.5284 - val_accuracy: 0.0241 - lr: 1.0000e-04\n",
      "Epoch 3/20\n",
      "2525/2525 [==============================] - 1950s 772ms/step - loss: 4.5143 - accuracy: 0.0276 - val_loss: 4.5020 - val_accuracy: 0.0317 - lr: 1.0000e-04\n",
      "Epoch 4/20\n",
      "2525/2525 [==============================] - 1967s 779ms/step - loss: 4.4874 - accuracy: 0.0315 - val_loss: 4.4776 - val_accuracy: 0.0325 - lr: 1.0000e-04\n",
      "Epoch 5/20\n",
      "2525/2525 [==============================] - 1944s 770ms/step - loss: 4.4638 - accuracy: 0.0347 - val_loss: 4.4609 - val_accuracy: 0.0359 - lr: 1.0000e-04\n",
      "Epoch 6/20\n",
      "2525/2525 [==============================] - 1945s 770ms/step - loss: 4.4414 - accuracy: 0.0382 - val_loss: 4.4409 - val_accuracy: 0.0377 - lr: 1.0000e-04\n",
      "Epoch 7/20\n",
      "2525/2525 [==============================] - 1948s 771ms/step - loss: 4.4196 - accuracy: 0.0420 - val_loss: 4.4174 - val_accuracy: 0.0399 - lr: 1.0000e-04\n",
      "Epoch 8/20\n",
      "2525/2525 [==============================] - 1975s 782ms/step - loss: 4.4005 - accuracy: 0.0449 - val_loss: 4.4004 - val_accuracy: 0.0402 - lr: 1.0000e-04\n",
      "Epoch 9/20\n",
      "2525/2525 [==============================] - 1988s 787ms/step - loss: 4.3809 - accuracy: 0.0464 - val_loss: 4.3804 - val_accuracy: 0.0483 - lr: 1.0000e-04\n",
      "Epoch 10/20\n",
      "2525/2525 [==============================] - 2024s 802ms/step - loss: 4.3647 - accuracy: 0.0486 - val_loss: 4.3674 - val_accuracy: 0.0498 - lr: 1.0000e-04\n",
      "Epoch 11/20\n",
      "2525/2525 [==============================] - 2023s 801ms/step - loss: 4.3475 - accuracy: 0.0522 - val_loss: 4.3557 - val_accuracy: 0.0498 - lr: 1.0000e-04\n",
      "Epoch 12/20\n",
      "2525/2525 [==============================] - 2010s 796ms/step - loss: 4.3342 - accuracy: 0.0519 - val_loss: 4.3395 - val_accuracy: 0.0533 - lr: 1.0000e-04\n",
      "Epoch 13/20\n",
      "2525/2525 [==============================] - 1996s 791ms/step - loss: 4.3217 - accuracy: 0.0541 - val_loss: 4.3335 - val_accuracy: 0.0518 - lr: 1.0000e-04\n",
      "Epoch 14/20\n",
      "2525/2525 [==============================] - 1988s 787ms/step - loss: 4.3111 - accuracy: 0.0559 - val_loss: 4.3170 - val_accuracy: 0.0556 - lr: 1.0000e-04\n",
      "Epoch 15/20\n",
      "2525/2525 [==============================] - 1984s 786ms/step - loss: 4.2993 - accuracy: 0.0571 - val_loss: 4.3088 - val_accuracy: 0.0559 - lr: 1.0000e-04\n",
      "Epoch 16/20\n",
      "2525/2525 [==============================] - 1980s 784ms/step - loss: 4.2872 - accuracy: 0.0592 - val_loss: 4.2943 - val_accuracy: 0.0594 - lr: 1.0000e-04\n",
      "Epoch 17/20\n",
      "2525/2525 [==============================] - 1896s 751ms/step - loss: 4.2802 - accuracy: 0.0607 - val_loss: 4.2974 - val_accuracy: 0.0570 - lr: 1.0000e-04\n",
      "Epoch 18/20\n",
      "2525/2525 [==============================] - 1874s 742ms/step - loss: 4.2719 - accuracy: 0.0621 - val_loss: 4.2806 - val_accuracy: 0.0621 - lr: 1.0000e-04\n",
      "Epoch 19/20\n",
      "2525/2525 [==============================] - 1892s 749ms/step - loss: 4.2634 - accuracy: 0.0626 - val_loss: 4.2844 - val_accuracy: 0.0571 - lr: 1.0000e-04\n",
      "Epoch 20/20\n",
      "2525/2525 [==============================] - ETA: 0s - loss: 4.2567 - accuracy: 0.0641\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "2525/2525 [==============================] - 1912s 757ms/step - loss: 4.2567 - accuracy: 0.0641 - val_loss: 4.2820 - val_accuracy: 0.0589 - lr: 1.0000e-04\n",
      "300 calories per slice\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# Set the dataset path\n",
    "dataset_path = \"C:/Study Material/Iternship/Prodigy/Task 5/archive/food-101/food-101/\"\n",
    "\n",
    "# Paths to meta files\n",
    "train_file = os.path.join(dataset_path, \"meta/train.txt\")\n",
    "test_file = os.path.join(dataset_path, \"meta/test.txt\")\n",
    "\n",
    "# Function to load file paths from meta files\n",
    "def load_file_paths(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        return [line.strip() for line in file.readlines()]\n",
    "\n",
    "train_paths = load_file_paths(train_file)\n",
    "test_paths = load_file_paths(test_file)\n",
    "\n",
    "# Create a dictionary to map image paths to class names\n",
    "def create_class_mapping(paths):\n",
    "    class_mapping = {}\n",
    "    for path in paths:\n",
    "        class_name = path.split('/')[0]\n",
    "        class_mapping[path] = class_name\n",
    "    return class_mapping\n",
    "\n",
    "train_mapping = create_class_mapping(train_paths)\n",
    "test_mapping = create_class_mapping(test_paths)\n",
    "\n",
    "# Load classes and labels\n",
    "with open(os.path.join(dataset_path, \"meta/classes.txt\"), \"r\") as file:\n",
    "    class_names = [line.strip() for line in file.readlines()]\n",
    "\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# Data Augmentation\n",
    "batch_size = 32\n",
    "image_size = (160, 160)  # Consider resizing further if necessary\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0,\n",
    "    rotation_range=15,  # Adjusted rotation range\n",
    "    width_shift_range=0.1,  # Adjusted shift range\n",
    "    height_shift_range=0.1,  # Adjusted shift range\n",
    "    shear_range=0.1,  # Adjusted shear range\n",
    "    zoom_range=0.1,  # Adjusted zoom range\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.9, 1.1],\n",
    "    fill_mode=\"nearest\",\n",
    "    validation_split=0.2  # Set aside 20% of data for validation\n",
    ")\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    os.path.join(dataset_path, \"images\"),\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    os.path.join(dataset_path, \"images\"),\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Load pre-trained ResNet50 model + higher level layers\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(image_size[0], image_size[1], 3))\n",
    "\n",
    "# Adding custom layers\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Final model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze the layers of the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-7, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=20,  # Start with more epochs, early stopping will handle the rest\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[reduce_lr, early_stopping]\n",
    ")\n",
    "\n",
    "# Calorie estimation dictionary (Example values, modify as needed)\n",
    "calorie_dict = {\n",
    "    'Apple Pie': '300 calories per slice',\n",
    "    'Baby Back Ribs': '500 calories per serving',\n",
    "    'Baklava': '350 calories per piece',\n",
    "    'Beef Carpaccio': '150 calories per serving',\n",
    "    'Beef Tartare': '200 calories per serving',\n",
    "    'Beet Salad': '150 calories per serving',\n",
    "    'Beignets': '250 calories per piece',\n",
    "    'Bibimbap': '550 calories per bowl',\n",
    "    'Bread Pudding': '400 calories per serving',\n",
    "    'Breakfast Burrito': '600 calories per burrito',\n",
    "    'Bruschetta': '150 calories per piece',\n",
    "    'Caesar Salad': '350 calories per serving',\n",
    "    'Cannoli': '250 calories per piece',\n",
    "    'Caprese Salad': '300 calories per serving',\n",
    "    'Carrot Cake': '350 calories per slice',\n",
    "    'Ceviche': '200 calories per serving',\n",
    "    'Cheesecake': '450 calories per slice',\n",
    "    'Cheese Plate': '600 calories per plate',\n",
    "    'Chicken Curry': '450 calories per serving',\n",
    "    'Chicken Quesadilla': '500 calories per serving',\n",
    "    'Chicken Wings': '430 calories per serving (about 6 wings)',\n",
    "    'Chocolate Cake': '500 calories per slice',\n",
    "    'Chocolate Mousse': '350 calories per serving',\n",
    "    'Churros': '250 calories per serving (about 2-3 churros)',\n",
    "    'Clam Chowder': '200 calories per bowl',\n",
    "    'Club Sandwich': '600 calories per sandwich',\n",
    "    'Crab Cakes': '300 calories per serving (about 2 cakes)',\n",
    "    'Creme Brulee': '300 calories per serving',\n",
    "    'Croque Madame': '550 calories per sandwich',\n",
    "    'Cup Cakes': '350 calories per cupcake',\n",
    "    'Deviled Eggs': '200 calories per 2 halves',\n",
    "    'Donuts': '300 calories per donut',\n",
    "    'Dumplings': '250 calories per serving (about 6 dumplings)',\n",
    "    'Edamame': '120 calories per serving (about 1 cup)',\n",
    "    'Eggs Benedict': '400 calories per serving',\n",
    "    'Escargots': '250 calories per serving',\n",
    "    'Falafel': '350 calories per serving (about 3 pieces)',\n",
    "    'Filet Mignon': '450 calories per serving (about 6 oz)',\n",
    "    'Fish and Chips': '600 calories per serving',\n",
    "    'Foie Gras': '400 calories per serving',\n",
    "    'French Fries': '300 calories per serving (about 1 cup)',\n",
    "    'French Onion Soup': '300 calories per bowl',\n",
    "    'French Toast': '400 calories per serving (about 2 slices)',\n",
    "    'Fried Calamari': '300 calories per serving',\n",
    "    'Fried Rice': '400 calories per serving',\n",
    "    'Frozen Yogurt': '200 calories per serving (about 1/2 cup)',\n",
    "    'Garlic Bread': '150 calories per slice',\n",
    "    'Gnocchi': '250 calories per serving',\n",
    "    'Greek Salad': '200 calories per serving',\n",
    "    'Grilled Cheese Sandwich': '400 calories per sandwich',\n",
    "    'Grilled Salmon': '350 calories per serving (about 6 oz)',\n",
    "    'Guacamole': '250 calories per serving (about 1/2 cup)',\n",
    "    'Gyoza': '200 calories per serving (about 5 pieces)',\n",
    "    'Hamburger': '500 calories per burger',\n",
    "    'Hot and Sour Soup': '150 calories per bowl',\n",
    "    'Hot Dog': '300 calories per hot dog',\n",
    "    'Huevos Rancheros': '550 calories per serving',\n",
    "    'Hummus': '180 calories per serving (about 1/4 cup)',\n",
    "    'Ice Cream': '200 calories per serving (about 1/2 cup)',\n",
    "    'Lasagna': '600 calories per serving',\n",
    "    'Lobster Bisque': '350 calories per bowl',\n",
    "    'Lobster Roll Sandwich': '400 calories per sandwich',\n",
    "    'Macaroni and Cheese': '450 calories per serving',\n",
    "    'Macarons': '150 calories per macaron',\n",
    "    'Miso Soup': '100 calories per bowl',\n",
    "    'Mussels': '250 calories per serving',\n",
    "    'Nachos': '600 calories per serving',\n",
    "    'Omelette': '300 calories per omelette',\n",
    "    'Onion Rings': '400 calories per serving',\n",
    "    'Oysters': '100 calories per serving (about 6 oysters)',\n",
    "    'Pad Thai': '600 calories per serving',\n",
    "    'Paella': '500 calories per serving',\n",
    "    'Pancakes': '350 calories per serving (about 2 pancakes)',\n",
    "    'Panna Cotta': '300 calories per serving',\n",
    "    'Peking Duck': '600 calories per serving',\n",
    "    'Pho': '450 calories per bowl',\n",
    "    'Pizza': '300 calories per slice',\n",
    "    'Pork Chop': '400 calories per serving (about 6 oz)',\n",
    "    'Poutine': '600 calories per serving',\n",
    "    'Prime Rib': '750 calories per serving (about 10 oz)',\n",
    "    'Pulled Pork Sandwich': '500 calories per sandwich',\n",
    "    'Ramen': '450 calories per bowl',\n",
    "    'Ravioli': '350 calories per serving',\n",
    "    'Red Velvet Cake': '400 calories per slice',\n",
    "    'Risotto': '400 calories per serving',\n",
    "    'Samosa': '200 calories per piece',\n",
    "    'Sashimi': '150 calories per serving',\n",
    "    'Scallops': '250 calories per serving',\n",
    "    'Shrimp Cocktail': '200 calories per serving',\n",
    "    'Sushi': '200 calories per serving (about 8 pieces)',\n",
    "    'Tacos': '300 calories per taco',\n",
    "    'Tamales': '400 calories per serving',\n",
    "    'Tiramisu': '350 calories per serving',\n",
    "    'Tom Yum Soup': '150 calories per bowl',\n",
    "    'Vegetable Soup': '100 calories per bowl',\n",
    "    'Waffles': '350 calories per serving (about 2 waffles)',\n",
    "    'Wings': '400 calories per serving (about 6 wings)'\n",
    "}\n",
    "\n",
    "# Example usage\n",
    "def estimate_calories(food_item):\n",
    "    return calorie_dict.get(food_item, \"Calorie information not available\")\n",
    "\n",
    "# Test the calorie estimation function\n",
    "print(estimate_calories('Pizza'))  # Example test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze some layers of the base model for incremental learning\n",
    "for layer in base_model.layers[-30:]:  # Unfreeze last 30 layers, adjust as needed\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompile the model with a lower learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Continue training the model with additional epochs\n",
    "additional_epochs = 25\n",
    "history_incremental = model.fit(\n",
    "    train_generator,\n",
    "    epochs=additional_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[reduce_lr, early_stopping]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model's accuracy was no longer improving I had to manually stop the training and after stopping the training I saved the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the updated model after additional training\n",
    "model.save('incremental_food_recognition_model.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally I trained it for further more epochs to check the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2525/2525 [==============================] - 3216s 1s/step - loss: 3.4703 - accuracy: 0.1898 - val_loss: 3.6408 - val_accuracy: 0.1626 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the previously saved model\n",
    "model = load_model('incremental_food_recognition_model.keras')\n",
    "\n",
    "# Optionally unfreeze more layers for fine-tuning\n",
    "for layer in model.layers[-30:]:  # Unfreeze the last 30 layers, adjust as needed\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompile the model with a lower learning rate for fine-tuning\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Continue training the model with additional epochs\n",
    "additional_epochs = 1  # Set the number of additional epochs\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=additional_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[reduce_lr, early_stopping]\n",
    ")\n",
    "\n",
    "# Save the model again after additional training\n",
    "model.save('incremental_food_recognition_model_v2.keras')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
